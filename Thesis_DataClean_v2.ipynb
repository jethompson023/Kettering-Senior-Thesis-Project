{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates to script for version 2\n",
    "\n",
    "1.   Using PySpark & Spark Pandas API for better performance & speed\n",
    "2.   Using Azure Cousmos DB to send cleaned data to PowerBI Dashboard\n",
    "3.   Minor usage of Numba Framework to speed up some methods within this script\n",
    "4.   Potential Trials of Time-Series Machine Learning Modeling of Data \n",
    "5.   Trials with using the Libra Library for potential simple Machine Learning development\n",
    "6.   Determine if using Azure Synapse makes sense for this project for giving no-code manipulation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website Links & Documentation for all program enhancements\n",
    "\n",
    "1.   PySpark Examples - https://github.com/apache/spark/tree/f74867bddf/examples/src/main/python\n",
    "2.   Azure Cosmos DB - https://docs.microsoft.com/en-us/azure/cosmos-db/sql/create-sql-api-python\n",
    "2.   Azure Cosmos DB Github Demo - https://github.com/Azure-Samples/azure-cosmos-db-python-getting-started/blob/main/cosmos_get_started.py\n",
    "3.   Numba - https://numba.pydata.org/\n",
    "4.   Time-Series Machine Learning Algos - https://www.advancinganalytics.co.uk/blog/2021/06/22/10-incredibly-useful-time-series-forecasting-algorithms\n",
    "5.   Libra - https://libradocs.org/\n",
    "6.   Azure Synapse & Cosmos DB Integration - https://docs.microsoft.com/en-us/azure/cosmos-db/synapse-link\n",
    "7.   Spark SQL & Python - https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "8.   DeepHaven Streaming Dataframe - https://deephaven.io/?utm_term=streaming%20data&utm_campaign=Website+Traffic+Q4+2022&utm_source=adwords&utm_medium=ppc&hsa_acc=4673439537&hsa_cam=18322365595&hsa_grp=139855244374&hsa_ad=621564846172&hsa_src=g&hsa_tgt=kwd-161093182&hsa_kw=streaming%20data&hsa_mt=p&hsa_net=adwords&hsa_ver=3\n",
    "9.   Spark SQL & Python - https://spark.apache.org/docs/latest/\n",
    "10.  Google Sheets & Python - https://ploomber.io/blog/gsheets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "#all imports \n",
    "import gspread\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "import numba\n",
    "from numba import jit\n",
    "import os\n",
    "import json\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/18 22:49:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#setup spark env \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'get_all_records'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m rows \u001b[38;5;241m=\u001b[39m work_sheet\u001b[38;5;241m.\u001b[39mget_all_values()\n\u001b[1;32m      6\u001b[0m pou_data \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(rows)\n\u001b[0;32m----> 7\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_records\u001b[49m()\n",
      "File \u001b[0;32m~/micromamba/envs/py39/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1988\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[39m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1979\u001b[0m \n\u001b[1;32m   1980\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1985\u001b[0m \u001b[39m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m-> 1988\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1989\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name)\n\u001b[1;32m   1990\u001b[0m     )\n\u001b[1;32m   1991\u001b[0m jc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mapply(name)\n\u001b[1;32m   1992\u001b[0m \u001b[39mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'get_all_records'"
     ]
    }
   ],
   "source": [
    "#Import the data from google sheets\n",
    "\"\"\"\n",
    "sa = gspread.service_account(filename=\"/com.docker.devenvironments.code/kuthesisdataclean-558333f9362c.json\")\n",
    "sheet = sa.open(\"POU_Unclean_Data\")\n",
    "work_sheet = sheet.worksheet(\"POU\")\n",
    "rows = work_sheet.get_all_values()\n",
    "pou_data = ps.DataFrame.from_records(rows)\n",
    "spark_df = spark.createDataFrame(rows).get_all_records()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call to the dataset within Google Sheets\n",
    "sa = gspread.service_account(filename=\"/com.docker.devenvironments.code/kuthesisdataclean-558333f9362c.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a method that return the values from the google sheet\n",
    "@jit\n",
    "def pull_df(sa):\n",
    "    sheet = sa.open(\"POU_Unclean_Data\")\n",
    "    work_sheet = sheet.worksheet(\"POU\")\n",
    "    df = ps.Dataframe(work_sheet.get_all_records())\n",
    "    spark_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2393/4249707886.py:2: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"pull_df\" failed type inference due to: Untyped global name 'spark': Cannot determine Numba type of <class 'pyspark.sql.session.SparkSession'>\n",
      "\n",
      "File \"tmp/ipykernel_2393/4249707886.py\", line 7:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  @jit\n",
      "/home/vscode/micromamba/envs/py39/lib/python3.10/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning: Function \"pull_df\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"tmp/ipykernel_2393/4249707886.py\", line 2:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "/home/vscode/micromamba/envs/py39/lib/python3.10/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"tmp/ipykernel_2393/4249707886.py\", line 2:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.pandas' has no attribute 'Dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[43mpull_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.pandas' has no attribute 'Dataframe'"
     ]
    }
   ],
   "source": [
    "spark_df = pull_df(sa)\n",
    "#spark_df = pull_df(spark_df)\n",
    "#spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new spark dataframe to use for analysis\n",
    "@jit\n",
    "def create_df():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Create Spark Dataframe with the data within the Google Sheet\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#spark_df = spark.createDataFrame(pou_data).toDF(columns_*)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#val dfFromRDD2 = spark.createDataFrame(rdd).toDF(columns:_*)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#spark_df.head()\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimplicits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spark'"
     ]
    }
   ],
   "source": [
    "#Create Spark Dataframe with the data within the Google Sheet\n",
    "#spark_df = spark.createDataFrame(pou_data).toDF(columns_*)\n",
    "#val dfFromRDD2 = spark.createDataFrame(rdd).toDF(columns:_*)\n",
    "#spark_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the dataframe so we can start working on using it for data analysis\n",
    "#Next we are going to format the rows to allow for the data to get into a data frame\n",
    "spark_df  = spark_df.drop(labels=0, axis=0)\n",
    "spark_df  = spark_df.reset_index(drop=True)\n",
    "spark_df.columns = spark_df.loc[0]\n",
    "spark_df  = spark_df.loc[1:].reset_index(drop=True)\n",
    "spark_df.drop_duplicates(subset=\"Serial Number\", keep='last')\n",
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def format_df(df):\n",
    "    df = df.drop(labels=0, axis=0)\n",
    "    df = df.index(drop=True)\n",
    "    df.columns = df.collect()[0]\n",
    "    df = df[1:].collect(drop=True)\n",
    "    df.drop_duplicates(subset=\"Serial Number\", keeps='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m format_df(\u001b[43mspark_df\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark_df' is not defined"
     ]
    }
   ],
   "source": [
    "spark_df = format_df(spark_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39b3ec054d787909dbf58ca94621dbabd0362452353e65f29fb0758a2d947852"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
