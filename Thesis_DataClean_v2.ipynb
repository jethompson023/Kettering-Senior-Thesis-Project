{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates to script for version 2\n",
    "\n",
    "1.   Using PySpark & Spark Pandas API for better performance & speed\n",
    "2.   Using Azure Cousmos DB to send cleaned data to PowerBI Dashboard\n",
    "3.   Minor usage of Numba Framework to speed up some methods within this script\n",
    "4.   Potential Trials of Time-Series Machine Learning Modeling of Data \n",
    "5.   Trials with using the Libra Library for potential simple Machine Learning development\n",
    "6.   Determine if using Azure Synapse makes sense for this project for giving no-code manipulation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website Links & Documentation for all program enhancements\n",
    "\n",
    "1.   PySpark Examples - https://github.com/apache/spark/tree/f74867bddf/examples/src/main/python\n",
    "2.   Azure Cosmos DB - https://docs.microsoft.com/en-us/azure/cosmos-db/sql/create-sql-api-python\n",
    "2.   Azure Cosmos DB Github Demo - https://github.com/Azure-Samples/azure-cosmos-db-python-getting-started/blob/main/cosmos_get_started.py\n",
    "3.   Numba - https://numba.pydata.org/\n",
    "4.   Time-Series Machine Learning Algos - https://www.advancinganalytics.co.uk/blog/2021/06/22/10-incredibly-useful-time-series-forecasting-algorithms\n",
    "5.   Libra - https://libradocs.org/\n",
    "6.   Azure Synapse & Cosmos DB Integration - https://docs.microsoft.com/en-us/azure/cosmos-db/synapse-link\n",
    "7.   Spark SQL & Python - https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "8.   DeepHaven Streaming Dataframe - https://deephaven.io/?utm_term=streaming%20data&utm_campaign=Website+Traffic+Q4+2022&utm_source=adwords&utm_medium=ppc&hsa_acc=4673439537&hsa_cam=18322365595&hsa_grp=139855244374&hsa_ad=621564846172&hsa_src=g&hsa_tgt=kwd-161093182&hsa_kw=streaming%20data&hsa_mt=p&hsa_net=adwords&hsa_ver=3\n",
    "9.   Spark SQL & Python - https://spark.apache.org/docs/latest/\n",
    "10.  Google Sheets & Python - https://ploomber.io/blog/gsheets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports \n",
    "import gspread\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "import numba\n",
    "from numba import jit\n",
    "import os\n",
    "import json\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup spark env & import spark variables\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call to the dataset within Google Sheets\n",
    "sa = gspread.service_account(filename=\"/com.docker.devenvironments.code/kuthesisdataclean-558333f9362c.json\")\n",
    "sheet = sa.open(\"POU_Unclean_Data\")\n",
    "work_sheet = sheet.worksheet(\"POU\")\n",
    "rows = work_sheet.get_all_values()\n",
    "#rows = rows.get_all_records()\n",
    "df = spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/28 22:07:07 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1422574 ms exceeds timeout 120000 ms\n",
      "22/09/28 22:07:07 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the dataframe so we can start working on using it for data analysis\n",
    "#Next we are going to format the rows to allow for the data to get into a data frame\n",
    "\"\"\"\n",
    "df  = df.drop(labels=0, axis=0)\n",
    "df  = df.reset_index(drop=True)\n",
    "df.columns = df.loc[0]\n",
    "df  = df.loc[1:].reset_index(drop=True)\n",
    "df.drop_duplicates(subset=\"Serial Number\", keep='last')\n",
    "df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def format_df(df):\n",
    "    df = df.drop(labels=0, axis=0)\n",
    "    df = df.index(drop=True)\n",
    "    df.columns = df.collect()[0]\n",
    "    df = df[1:].collect(drop=True)\n",
    "    df.drop_duplicates(subset=\"Serial Number\", keeps='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1155/472170987.py:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"format_df\" failed type inference due to: non-precise type pyobject\n",
      "During: typing of argument at /tmp/ipykernel_1155/472170987.py (3)\n",
      "\n",
      "File \"tmp/ipykernel_1155/472170987.py\", line 3:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  @jit\n",
      "/home/vscode/micromamba/envs/py39/lib/python3.10/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning: Function \"format_df\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"tmp/ipykernel_1155/472170987.py\", line 1:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "/home/vscode/micromamba/envs/py39/lib/python3.10/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"tmp/ipykernel_1155/472170987.py\", line 1:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataFrame.drop() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mformat_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: DataFrame.drop() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "df = format_df(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39b3ec054d787909dbf58ca94621dbabd0362452353e65f29fb0758a2d947852"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
