{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates to script for version 2\n",
    "\n",
    "1.   Using PySpark & Spark Pandas API for better performance & speed\n",
    "2.   Using Azure Cousmos DB to send cleaned data to PowerBI Dashboard\n",
    "3.   Minor usage of Numba Framework to speed up some methods within this script\n",
    "4.   Potential Trials of Time-Series Machine Learning Modeling of Data \n",
    "5.   Trials with using the Libra Library for potential simple Machine Learning development\n",
    "6.   Determine if using Azure Synapse makes sense for this project for giving no-code manipulation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website Links & Documentation for all program enhancements\n",
    "\n",
    "1.   PySpark Examples - https://github.com/apache/spark/tree/f74867bddf/examples/src/main/python\n",
    "2.   Azure Cosmos DB - https://docs.microsoft.com/en-us/azure/cosmos-db/sql/create-sql-api-python\n",
    "2.   Azure Cosmos DB Github Demo - https://github.com/Azure-Samples/azure-cosmos-db-python-getting-started/blob/main/cosmos_get_started.py\n",
    "3.   Numba - https://numba.pydata.org/\n",
    "4.   Time-Series Machine Learning Algos - https://www.advancinganalytics.co.uk/blog/2021/06/22/10-incredibly-useful-time-series-forecasting-algorithms\n",
    "5.   Libra - https://libradocs.org/\n",
    "6.   Azure Synapse & Cosmos DB Integration - https://docs.microsoft.com/en-us/azure/cosmos-db/synapse-link\n",
    "7.   Spark SQL & Python - https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "8.   DeepHaven Streaming Dataframe - https://deephaven.io/?utm_term=streaming%20data&utm_campaign=Website+Traffic+Q4+2022&utm_source=adwords&utm_medium=ppc&hsa_acc=4673439537&hsa_cam=18322365595&hsa_grp=139855244374&hsa_ad=621564846172&hsa_src=g&hsa_tgt=kwd-161093182&hsa_kw=streaming%20data&hsa_mt=p&hsa_net=adwords&hsa_ver=3\n",
    "9.   Spark SQL & Python - https://spark.apache.org/docs/latest/\n",
    "10.  Google Sheets & Python - https://ploomber.io/blog/gsheets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports \n",
    "import gspread\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "import numba\n",
    "from numba import jit\n",
    "import os\n",
    "import json\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data from google sheets\n",
    "sa = gspread.service_account(filename=\"/com.docker.devenvironments.code/kuthesisdataclean-558333f9362c.json\")\n",
    "sheet = sa.open(\"POU_Unclean_Data\")\n",
    "work_sheet = sheet.worksheet(\"POU\")\n",
    "rows = work_sheet.get_all_values()\n",
    "pou_data = ps.DataFrame.from_records(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup spark env \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1407407191.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [22], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    spark_df = spark.createDataFrame(pou_data).toDF(columns:_*)\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Create Spark Dataframe with the data within the Google Sheet\n",
    "spark_df = spark.createDataFrame(pou_data).toDF(columns:_*)\n",
    "#val dfFromRDD2 = spark.createDataFrame(rdd).toDF(columns:_*)\n",
    "spark_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the dataframe so we can start working on using it for data analysis\n",
    "#Next we are going to format the rows to allow for the data to get into a data frame\n",
    "spark_df  = spark_df.drop(labels=0, axis=0)\n",
    "spark_df  = spark_df.reset_index(drop=True)\n",
    "spark_df.columns = spark_df.loc[0]\n",
    "spark_df  = spark_df.loc[1:].reset_index(drop=True)\n",
    "spark_df.drop_duplicates(subset=\"Serial Number\", keep='last')\n",
    "spark_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39b3ec054d787909dbf58ca94621dbabd0362452353e65f29fb0758a2d947852"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
