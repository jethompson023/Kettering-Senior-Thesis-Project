{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates to script for version 2\n",
    "\n",
    "1.   Using PySpark & Pandas for better performance & speed\n",
    "2.   Using Azure Cousmos DB to send cleaned data to PowerBI Dashboard\n",
    "3.   Minor usage of Numba Framework to speed up some methods within this script\n",
    "4.   Potential Trials of Time-Series Machine Learning Modeling of Data \n",
    "5.   Trials with using the Libra Library for potential simple Machine Learning development\n",
    "6.   Determine if using Azure Synapse makes sense for this project for giving no-code manipulation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website Links & Documentation for all program enhancements\n",
    "\n",
    "1.   PySpark Examples - https://github.com/apache/spark/tree/f74867bddf/examples/src/main/python\n",
    "2.   Azure Cosmos DB - https://docs.microsoft.com/en-us/azure/cosmos-db/sql/create-sql-api-python\n",
    "2.   Azure Cosmos DB Github Demo - https://github.com/Azure-Samples/azure-cosmos-db-python-getting-started/blob/main/cosmos_get_started.py\n",
    "3.   Numba - https://numba.pydata.org/\n",
    "4.   Time-Series Machine Learning Algos - https://www.advancinganalytics.co.uk/blog/2021/06/22/10-incredibly-useful-time-series-forecasting-algorithms\n",
    "5.   Libra - https://libradocs.org/\n",
    "6.   Azure Synapse & Cosmos DB Integration - https://docs.microsoft.com/en-us/azure/cosmos-db/synapse-link\n",
    "7.   Spark SQL & Python - https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "8.   DeepHaven Streaming Dataframe - https://deephaven.io/?utm_term=streaming%20data&utm_campaign=Website+Traffic+Q4+2022&utm_source=adwords&utm_medium=ppc&hsa_acc=4673439537&hsa_cam=18322365595&hsa_grp=139855244374&hsa_ad=621564846172&hsa_src=g&hsa_tgt=kwd-161093182&hsa_kw=streaming%20data&hsa_mt=p&hsa_net=adwords&hsa_ver=3\n",
    "9.   Spark SQL & Python - https://spark.apache.org/docs/latest/\n",
    "10.  Google Sheets & Python - https://ploomber.io/blog/gsheets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports \n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import numba\n",
    "from numba import jit\n",
    "import os\n",
    "import json\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data from google sheets\n",
    "sa = gspread.service_account(filename=\"/com.docker.devenvironments.code/kuthesisdataclean-558333f9362c.json\")\n",
    "sheet = sa.open(\"POU_Unclean_Data\")\n",
    "work_sheet = sheet.worksheet(\"POU\")\n",
    "rows = work_sheet.get_all_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup spark env \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.drop() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(rows)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Format the dataframe so we can start working on using it for data analysis\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Next we are going to format the rows to allow for the data to get into a data frame\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m spark_df  \u001b[38;5;241m=\u001b[39m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m spark_df  \u001b[38;5;241m=\u001b[39m spark_df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m spark_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m spark_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: DataFrame.drop() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "#Create Spark Dataframe with the data within the Google Sheet\n",
    "spark_df = spark.createDataFrame(rows)\n",
    "\n",
    "#Format the dataframe so we can start working on using it for data analysis\n",
    "#Next we are going to format the rows to allow for the data to get into a data frame\n",
    "spark_df  = spark_df.drop(labels=0, axis=0)\n",
    "spark_df  = spark_df.reset_index(drop=True)\n",
    "spark_df.columns = spark_df.loc[0]\n",
    "spark_df  = spark_df.loc[1:].reset_index(drop=True)\n",
    "spark_df.drop_duplicates(subset=\"Serial Number\", keep='last')\n",
    "spark_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39b3ec054d787909dbf58ca94621dbabd0362452353e65f29fb0758a2d947852"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
