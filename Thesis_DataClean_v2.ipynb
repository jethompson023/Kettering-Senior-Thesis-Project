{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates to script for version 2\n",
    "\n",
    "1.   Using PySpark & Spark Pandas API for better performance & speed\n",
    "2.   Using Azure Cousmos DB to send cleaned data to PowerBI Dashboard\n",
    "3.   Minor usage of Numba Framework to speed up some methods within this script\n",
    "4.   Potential Trials of Time-Series Machine Learning Modeling of Data \n",
    "5.   Trials with using the Libra Library for potential simple Machine Learning development\n",
    "6.   Determine if using Azure Synapse makes sense for this project for giving no-code manipulation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website Links & Documentation for all program enhancements\n",
    "\n",
    "1.   PySpark Examples - https://github.com/apache/spark/tree/f74867bddf/examples/src/main/python\n",
    "2.   Azure Cosmos DB - https://docs.microsoft.com/en-us/azure/cosmos-db/sql/create-sql-api-python\n",
    "2.   Azure Cosmos DB Github Demo - https://github.com/Azure-Samples/azure-cosmos-db-python-getting-started/blob/main/cosmos_get_started.py\n",
    "3.   Numba - https://numba.pydata.org/\n",
    "4.   Time-Series Machine Learning Algos - https://www.advancinganalytics.co.uk/blog/2021/06/22/10-incredibly-useful-time-series-forecasting-algorithms\n",
    "5.   Libra - https://libradocs.org/\n",
    "6.   Azure Synapse & Cosmos DB Integration - https://docs.microsoft.com/en-us/azure/cosmos-db/synapse-link\n",
    "7.   Spark SQL & Python - https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "8.   DeepHaven Streaming Dataframe - https://deephaven.io/?utm_term=streaming%20data&utm_campaign=Website+Traffic+Q4+2022&utm_source=adwords&utm_medium=ppc&hsa_acc=4673439537&hsa_cam=18322365595&hsa_grp=139855244374&hsa_ad=621564846172&hsa_src=g&hsa_tgt=kwd-161093182&hsa_kw=streaming%20data&hsa_mt=p&hsa_net=adwords&hsa_ver=3\n",
    "9.   Spark SQL & Python - https://spark.apache.org/docs/latest/\n",
    "10.  Google Sheets & Python - https://ploomber.io/blog/gsheets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "#all imports \n",
    "import gspread\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "import numba\n",
    "from numba import jit\n",
    "import os\n",
    "import json\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/29 01:02:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#setup spark env & import spark variables\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call to the dataset within Google Sheets\n",
    "sa = gspread.service_account(filename=\"/com.docker.devenvironments.code/kuthesisdataclean-558333f9362c.json\")\n",
    "sheet = sa.open(\"POU_Unclean_Data\")\n",
    "work_sheet = sheet.worksheet(\"POU\")\n",
    "rows = work_sheet.get_all_values()\n",
    "column_names = ['Item_no', 'Item Description', 'Quantity', 'Status', 'Serial Number', 'Reason Code', 'Shortage Notes', 'Team Found', 'Team Responsible', 'Data Found', 'Time Found Pacific', 'Data Found Pacific', 'SHORT_FOUND_EMP_ID', 'Date_chassis_start', 'date_veh_offln', 'date_mfg_rlse', 'PRIM_SHOP_ADDR', 'SCD_SHOP_ADDR_1', 'SCD_SHOP_ADDR_2', 'SCD_SHOP_ADDR_3']\n",
    "#rows = rows.get_all_records()\n",
    "df = spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send data to the Cosmos DB instance\n",
    "cosmosEndpoint = \"https://REPLACEME.documents.azure.com:443/\"\n",
    "cosmosMasterKey = \"REPLACEME\"\n",
    "cosmosDatabaseName = \"sampleDB\"\n",
    "cosmosContainerName = \"sampleContainer\"\n",
    "\n",
    "cfg = {\n",
    "  \"spark.cosmos.accountEndpoint\" : cosmosEndpoint,\n",
    "  \"spark.cosmos.accountKey\" : cosmosMasterKey,\n",
    "  \"spark.cosmos.database\" : cosmosDatabaseName,\n",
    "  \"spark.cosmos.container\" : cosmosContainerName,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39b3ec054d787909dbf58ca94621dbabd0362452353e65f29fb0758a2d947852"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
